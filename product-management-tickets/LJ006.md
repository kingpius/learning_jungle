## üéüÔ∏è Product Ticket: LJ-006 ‚Äî AI Diagnostic Question Generation (Maths Only)

---

### üìå Ticket ID

LJ-006

---

### üßë‚Äçüíº Ownership & Roles

* **Primary Owner (Full Stack Engineer):** Backend / AI Integration Engineer
* **Product Manager:** PM (scope control, acceptance)
* **Test Owner:** Test Engineer (AI output validation & determinism checks)
* **Design Owner:** ‚ùå Not required for this ticket

---

### üéØ Objective

Introduce **AI-powered generation of Maths diagnostic questions** aligned to the **UK National Curriculum**, tailored to a child‚Äôs **age and year group**.

This ticket enables the platform to automatically generate questions for the **first diagnostic test**, without introducing adaptive learning loops yet.

‚ö†Ô∏è This ticket focuses on **generation only**, not learning paths, rankings, or UI polish.

---

### üß© In Scope (MVP ONLY)

* AI-generated Maths diagnostic questions
* Curriculum-aligned prompts (UK)
* Difficulty aligned to child age & year group
* Fixed number of questions per diagnostic test
* Deterministic generation strategy (controlled randomness)

---

### üö´ Out of Scope (Explicitly Excluded)

* English or Science AI generation
* Adaptive difficulty during a test
* Long-term learning personalization
* Rankings or streak logic
* UI polish or animations
* Open-ended answers (multiple choice only for MVP)

---

### üß† Functional Requirements

#### 1. AI Question Generator Service

The system must implement an **AI Question Generator** capable of producing Maths diagnostic questions using an LLM.

Inputs:

* Child age
* Child year group
* Subject = Maths
* Number of questions (configurable, default 10‚Äì15)

Outputs (per question):

* Question text
* Multiple-choice options (A‚ÄìD)
* Correct answer index
* Difficulty tag (e.g. easy / medium / hard)

---

#### 2. Curriculum Alignment

* Questions must align to **UK National Curriculum expectations** for the specified year group
* Prompts must explicitly instruct the AI to:

  * Avoid content above the child‚Äôs year level
  * Focus on core Maths strands (number, arithmetic, basic problem solving)

---

#### 3. Determinism & Safety

* AI generation must be **repeatable for the same inputs** (seeded or cached)
* Generated content must be:

  * Age-appropriate
  * Free of inappropriate language
  * Free of non-curriculum material

---

#### 4. Integration with Diagnostic Test (LJ-005)

* Generated questions must be attached to a Diagnostic Test instance
* Question generation occurs **once per diagnostic test creation**
* Re-running generation must not overwrite completed tests
* Service behaviour must follow `product-management-tickets/aiservice.md`

---

### üß™ Acceptance Criteria

| Scenario                   | Expected Result         |
| -------------------------- | ----------------------- |
| Maths diagnostic requested | Questions generated     |
| Child age/year provided    | Difficulty appropriate  |
| Same inputs reused         | Same questions returned |
| Test already completed     | No regeneration         |
| Invalid subject            | Rejected                |

All criteria must pass for acceptance.

---

### üîê Non-Functional Requirements

* Must comply with existing data models (LJ-004, LJ-005)
* AI calls must be isolated in a service layer
* Prompt logic must be versioned and testable
* No direct AI calls from views

---

### üèóÔ∏è Technical Notes

* Use a dedicated AI service module
* Store generated questions in the database
* Consider caching by (age, year, subject)
* Log prompts and responses for audit/debugging
* Follow AI guardrails in `product-management-tickets/aiservice.md`

---

### üß≠ Dependencies

* LJ-002 ‚Äî Parent Authentication ‚úÖ
* LJ-004 ‚Äî Child Profile Model & CRUD ‚úÖ
* LJ-005 ‚Äî Diagnostic Test Model & Completion Event ‚úÖ

---

### üèÅ Definition of Done

* Feature branch created: `feature/LJ-006-ai-diagnostics-maths`
* AI service implemented and integrated
* Generated questions persisted
* Acceptance criteria validated
* Pull Request opened, reviewed, and merged
* No scope creep introduced
* Ticket + AI service doc updated with implementation status

---

### üö¶ Priority

P0 ‚Äî Core MVP intelligence layer

---

### üîÑ Updates & Coordination Log

**Update 2025-12-17 ‚Äî PM**
- AI guardrail document published at `product-management-tickets/aiservice.md`. Full Stack Engineer must follow that architecture (ai module, deterministic seed, caching) and seek PM approval before changing prompt version `lj-ai-maths-v1`.
- **Full Stack Engineer:** deliver `ai/` module, question persistence, `ensure_maths_questions_for_test` service, and integration into DiagnosticTest creation flow. No AI calls from views; configuration via env only.
- **Test Engineer:** create deterministic mocks for AI responses, cover success/failure caching scenarios, and ensure logging/audit requirements per aiservice doc. Tests may not hit live AI endpoints.
- Next checkpoint: share service design + initial tests in this ticket before implementation lands in repo.

**Update 2025-12-17 ‚Äî PM**
- **Full Stack Engineer quick notes:** reuse `DiagnosticTest` + `DiagnosticQuestion` models (extend if needed), call `ensure_maths_questions_for_test()` immediately after test creation, and block regeneration when `test.questions.exists()` or `test.is_completed`. Log each AI attempt with prompt version + seed.
- **Test Engineer quick notes:** build fixtures for (success payload, schema error, provider timeout); assert service never writes partial data and logging occurs even on failure. Remember to mention the `python manage.py migrate` prerequisite in test docs.
- **QA cadence:** share initial screenshots/logs here before PR so PM can verify prompt + response logging compliance.

**Update 2025-12-17 ‚Äî Full Stack Engineer**
- Delivered `DiagnosticQuestion` + `AIRequestLog` models (with migrations) and wired deterministic service `ensure_maths_questions_for_test()` that builds the prompt via `ai/prompts.py`, calls `ai/maths_diagnostic.generate_maths_mcqs()`, logs prompt/response excerpts, and persists MCQs atomically.
- Added `ai/` package (prompts, providers, schemas, exceptions) and `assessments/diagnostics/services.py` per guardrail doc; views now invoke the service on Maths test creation and surface 4xx/5xx per validation vs provider errors.
- Acceptance tests expanded (`assessments/tests.py`) with deterministic fixtures covering happy path, idempotency, completed-test rejection, failure logging, and view-level handling. Suite green via `./venv/bin/python manage.py test assessments rewards`.
- Blocker: still unable to create `feature/LJ-006-ai-diagnostics-maths` locally because `.git/refs/heads/...` remains read-only (same issue noted on LJ-005). Need repo owner/DevOps to fix permissions so I can stage + push the AI work.

**Update 2025-12-17 ‚Äî Full Stack Engineer**
- Attempted `git checkout -b feature/LJ-006-ai-diagnostics-maths` again after LJ-005 push; `.git/refs/heads/feature/LJ-006-ai-diagnostics-maths.lock` still raises `Operation not permitted`.
- Requesting repo owner/DevOps to reapply write permissions (or create the branch server-side) so I can publish the LJ-006 implementation and open the PR per Definition of Done.

**Update 2025-12-17 ‚Äî Full Stack Engineer**
- Repo now contains `feature/LJ-006-ai-diagnostics-maths` (appears pre-created server side), but local Git still cannot write lock files (`.git/index.lock` permission denied), so I can‚Äôt check out the branch or stage commits.
- Need DevOps to fix `.git` ownership/permissions entirely (not just refs) so I can switch branches, stage the AI modules/migrations, and push per Definition of Done. Ready to publish as soon as the lock issue is cleared.

**Update 2025-12-17 ‚Äî Test Engineer**
- Validated AI service + Diagnostic Test integration using deterministic mocks; `python manage.py test assessments rewards` (10 tests) passes after running migrations. Coverage spans question persistence, service idempotency, AI failure handling, completed-test guard, and parent access. Evidence lives in `assessments/tests.py`.
- Confirmed logging + audit trail via `AIRequestLog` entries for both success and failure paths, and verified view rolls back partially created tests when AI fails (`assessments/views.py:35-81`).
- Outstanding risks: requirement ‚Äúsame inputs ‚Üí same questions‚Äù currently depends on upstream provider honoring the provided seed; there is no caching by `(age, year)` yet, so we should document/monitor in staging once live responses are enabled.
- Decision: **Go for merged implementation**, contingent on documenting the determinism caveat above and ensuring CI/devs run `python manage.py migrate` before executing the suite.

**Update 2025-12-17 ‚Äî PM**
- Permission blocker resolved; branch `feature/LJ-006-ai-diagnostics-maths` now exists. Full Stack Engineer to stage only LJ-006 deliverables (`ai/`, new models/migrations, assessments updates, tickets) and leave unrelated env files untouched.
- Before pushing, re-run `python manage.py migrate` + `python manage.py test assessments rewards` (capture output) and note the determinism caveat in the PR description per Test Engineer guidance.
- Once PR is up, drop the link + test command here for acceptance review. Test Engineer to double-check PR includes the migrate prerequisite and logging evidence.

**Update 2025-12-17 ‚Äî Full Stack Engineer**
- Branch pushed: `feature/LJ-006-ai-diagnostics-maths` and PR opened: <https://github.com/kingpius/learning_jungle/compare/feature/LJ-006-ai-diagnostics-maths?expand=1>. Description includes summary, acceptance mapping, determinism caveat (‚Äúseed enforced but true repeatability depends on provider; caching planned post-MVP‚Äù), and explicit reminder to run `python manage.py migrate` before tests. Test output attached: `python manage.py migrate && python manage.py test assessments rewards` (10 tests, green).
- Awaiting PM/Test review. Will monitor PR for feedback and keep aiservice.md + this ticket updated if prompts/logging change.

**Update 2025-12-17 ‚Äî PM**
- PR description snapshot for review:
  - **Summary:** Adds AI service module (prompt builder, provider wrapper, schema validation, deterministic seed), `assessments/diagnostics/services.py`, `DiagnosticQuestion` + `AIRequestLog` models/migration/admin wiring, and view/test updates so Maths tests generate/persist MCQs once with audit trail.
  - **Acceptance Criteria mapping:** maths diagnostics generate MCQs aligned to age/year, per-test calls are idempotent, completed tests reject regeneration, invalid AI responses bubble up with no partial data saved, logging captures prompt/seed/latency.
  - **Testing:** `python manage.py migrate` followed by `python manage.py test assessments rewards` (10 tests, green).
  - **Notes:** Determinism enforced via seed + per-test caching; full repeatability still depends on provider honoring seed. Caching by `(age, year)` deferred post-MVP. Include migrate prerequisite in runbooks.
- Reviewers can reference this entry instead of expanding the PR description.

**Update 2025-12-17 ‚Äî Full Stack Engineer**
- Post-merge verification complete. After resolving `config/urls.py` conflict with main, re-ran:
  - `python manage.py migrate assessments`
  - `python manage.py test assessments rewards`
- All 10 tests still green; no pending migrations. PR updated with comment confirming rerun so reviewers know merge didn‚Äôt introduce regressions.

**Final Update 2025-12-17 ‚Äî PM**
- PR merged to `main`; AI Maths diagnostics now live in trunk. Ensure staging deploy toggles `AI_PROVIDER` + `AI_API_KEY` as documented in `aiservice.md`.
- Post-merge checklist complete (migrations applied, runbook updated with determinism caveat). LJ-006 marked **Done**; future enhancements (e.g., caching by age/year or adding English/Science) require new tickets.

**Summary of Achievements**
- Implemented deterministic AI Maths question generation per `aiservice.md`, including `ai/` service layer, prompt versioning, provider wrapper, schema validation, and deterministic seed calculation.
- Added `DiagnosticQuestion` and `AIRequestLog` models with migrations/admin wiring, enabling per-test caching of MCQs and full audit/logging of every AI request (prompt excerpt, response excerpt, latency, status).
- Hooked generation into DiagnosticTest creation so Maths tests automatically receive MCQs; failures roll back the test record with clear errors, and completed tests/idempotent calls are guarded.
- Expanded automated tests to cover service success, idempotency, completed-test rejection, provider failures, logging, and view-level behavior; suite `python manage.py test assessments rewards` (10 tests) remains green post-merge.
- Documented prompt/service behavior, test instructions, determinism caveat, and migrate prerequisite in both the PR and this ticket for auditability.


