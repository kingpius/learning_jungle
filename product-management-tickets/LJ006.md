## ğŸŸï¸ Product Ticket: LJ-006 â€” AI Diagnostic Question Generation (Maths Only)

---

### ğŸ“Œ Ticket ID

LJ-006

---

### ğŸ§‘â€ğŸ’¼ Ownership & Roles

* **Primary Owner (Full Stack Engineer):** Backend / AI Integration Engineer
* **Product Manager:** PM (scope control, acceptance)
* **Test Owner:** Test Engineer (AI output validation & determinism checks)
* **Design Owner:** âŒ Not required for this ticket

---

### ğŸ¯ Objective

Introduce **AI-powered generation of Maths diagnostic questions** aligned to the **UK National Curriculum**, tailored to a childâ€™s **age and year group**.

This ticket enables the platform to automatically generate questions for the **first diagnostic test**, without introducing adaptive learning loops yet.

âš ï¸ This ticket focuses on **generation only**, not learning paths, rankings, or UI polish.

---

### ğŸ§© In Scope (MVP ONLY)

* AI-generated Maths diagnostic questions
* Curriculum-aligned prompts (UK)
* Difficulty aligned to child age & year group
* Fixed number of questions per diagnostic test
* Deterministic generation strategy (controlled randomness)

---

### ğŸš« Out of Scope (Explicitly Excluded)

* English or Science AI generation
* Adaptive difficulty during a test
* Long-term learning personalization
* Rankings or streak logic
* UI polish or animations
* Open-ended answers (multiple choice only for MVP)

---

### ğŸ§  Functional Requirements

#### 1. AI Question Generator Service

The system must implement an **AI Question Generator** capable of producing Maths diagnostic questions using an LLM.

Inputs:

* Child age
* Child year group
* Subject = Maths
* Number of questions (configurable, default 10â€“15)

Outputs (per question):

* Question text
* Multiple-choice options (Aâ€“D)
* Correct answer index
* Difficulty tag (e.g. easy / medium / hard)

---

#### 2. Curriculum Alignment

* Questions must align to **UK National Curriculum expectations** for the specified year group
* Prompts must explicitly instruct the AI to:

  * Avoid content above the childâ€™s year level
  * Focus on core Maths strands (number, arithmetic, basic problem solving)

---

#### 3. Determinism & Safety

* AI generation must be **repeatable for the same inputs** (seeded or cached)
* Generated content must be:

  * Age-appropriate
  * Free of inappropriate language
  * Free of non-curriculum material

---

#### 4. Integration with Diagnostic Test (LJ-005)

* Generated questions must be attached to a Diagnostic Test instance
* Question generation occurs **once per diagnostic test creation**
* Re-running generation must not overwrite completed tests
* Service behaviour must follow `product-management-tickets/aiservice.md`

---

### ğŸ§ª Acceptance Criteria

| Scenario                   | Expected Result         |
| -------------------------- | ----------------------- |
| Maths diagnostic requested | Questions generated     |
| Child age/year provided    | Difficulty appropriate  |
| Same inputs reused         | Same questions returned |
| Test already completed     | No regeneration         |
| Invalid subject            | Rejected                |

All criteria must pass for acceptance.

---

### ğŸ” Non-Functional Requirements

* Must comply with existing data models (LJ-004, LJ-005)
* AI calls must be isolated in a service layer
* Prompt logic must be versioned and testable
* No direct AI calls from views

---

### ğŸ—ï¸ Technical Notes

* Use a dedicated AI service module
* Store generated questions in the database
* Consider caching by (age, year, subject)
* Log prompts and responses for audit/debugging
* Follow AI guardrails in `product-management-tickets/aiservice.md`

---

### ğŸ§­ Dependencies

* LJ-002 â€” Parent Authentication âœ…
* LJ-004 â€” Child Profile Model & CRUD âœ…
* LJ-005 â€” Diagnostic Test Model & Completion Event âœ…

---

### ğŸ Definition of Done

* Feature branch created: `feature/LJ-006-ai-diagnostics-maths`
* AI service implemented and integrated
* Generated questions persisted
* Acceptance criteria validated
* Pull Request opened, reviewed, and merged
* No scope creep introduced
* Ticket + AI service doc updated with implementation status

---

### ğŸš¦ Priority

P0 â€” Core MVP intelligence layer

---

### ğŸ”„ Updates & Coordination Log

**Update 2025-12-17 â€” PM**
- AI guardrail document published at `product-management-tickets/aiservice.md`. Full Stack Engineer must follow that architecture (ai module, deterministic seed, caching) and seek PM approval before changing prompt version `lj-ai-maths-v1`.
- **Full Stack Engineer:** deliver `ai/` module, question persistence, `ensure_maths_questions_for_test` service, and integration into DiagnosticTest creation flow. No AI calls from views; configuration via env only.
- **Test Engineer:** create deterministic mocks for AI responses, cover success/failure caching scenarios, and ensure logging/audit requirements per aiservice doc. Tests may not hit live AI endpoints.
- Next checkpoint: share service design + initial tests in this ticket before implementation lands in repo.

**Update 2025-12-17 â€” PM**
- **Full Stack Engineer quick notes:** reuse `DiagnosticTest` + `DiagnosticQuestion` models (extend if needed), call `ensure_maths_questions_for_test()` immediately after test creation, and block regeneration when `test.questions.exists()` or `test.is_completed`. Log each AI attempt with prompt version + seed.
- **Test Engineer quick notes:** build fixtures for (success payload, schema error, provider timeout); assert service never writes partial data and logging occurs even on failure. Remember to mention the `python manage.py migrate` prerequisite in test docs.
- **QA cadence:** share initial screenshots/logs here before PR so PM can verify prompt + response logging compliance.

**Update 2025-12-17 â€” Full Stack Engineer**
- Delivered `DiagnosticQuestion` + `AIRequestLog` models (with migrations) and wired deterministic service `ensure_maths_questions_for_test()` that builds the prompt via `ai/prompts.py`, calls `ai/maths_diagnostic.generate_maths_mcqs()`, logs prompt/response excerpts, and persists MCQs atomically.
- Added `ai/` package (prompts, providers, schemas, exceptions) and `assessments/diagnostics/services.py` per guardrail doc; views now invoke the service on Maths test creation and surface 4xx/5xx per validation vs provider errors.
- Acceptance tests expanded (`assessments/tests.py`) with deterministic fixtures covering happy path, idempotency, completed-test rejection, failure logging, and view-level handling. Suite green via `./venv/bin/python manage.py test assessments rewards`.
- Blocker: still unable to create `feature/LJ-006-ai-diagnostics-maths` locally because `.git/refs/heads/...` remains read-only (same issue noted on LJ-005). Need repo owner/DevOps to fix permissions so I can stage + push the AI work.

**Update 2025-12-17 â€” Full Stack Engineer**
- Attempted `git checkout -b feature/LJ-006-ai-diagnostics-maths` again after LJ-005 push; `.git/refs/heads/feature/LJ-006-ai-diagnostics-maths.lock` still raises `Operation not permitted`.
- Requesting repo owner/DevOps to reapply write permissions (or create the branch server-side) so I can publish the LJ-006 implementation and open the PR per Definition of Done.

**Update 2025-12-17 â€” Full Stack Engineer**
- Repo now contains `feature/LJ-006-ai-diagnostics-maths` (appears pre-created server side), but local Git still cannot write lock files (`.git/index.lock` permission denied), so I canâ€™t check out the branch or stage commits.
- Need DevOps to fix `.git` ownership/permissions entirely (not just refs) so I can switch branches, stage the AI modules/migrations, and push per Definition of Done. Ready to publish as soon as the lock issue is cleared.

**Update 2025-12-17 â€” Test Engineer**
- Validated AI service + Diagnostic Test integration using deterministic mocks; `python manage.py test assessments rewards` (10 tests) passes after running migrations. Coverage spans question persistence, service idempotency, AI failure handling, completed-test guard, and parent access. Evidence lives in `assessments/tests.py`.
- Confirmed logging + audit trail via `AIRequestLog` entries for both success and failure paths, and verified view rolls back partially created tests when AI fails (`assessments/views.py:35-81`).
- Outstanding risks: requirement â€œsame inputs â†’ same questionsâ€ currently depends on upstream provider honoring the provided seed; there is no caching by `(age, year)` yet, so we should document/monitor in staging once live responses are enabled.
- Decision: **Go for merged implementation**, contingent on documenting the determinism caveat above and ensuring CI/devs run `python manage.py migrate` before executing the suite.

**Update 2025-12-17 â€” PM**
- Permission blocker resolved; branch `feature/LJ-006-ai-diagnostics-maths` now exists. Full Stack Engineer to stage only LJ-006 deliverables (`ai/`, new models/migrations, assessments updates, tickets) and leave unrelated env files untouched.
- Before pushing, re-run `python manage.py migrate` + `python manage.py test assessments rewards` (capture output) and note the determinism caveat in the PR description per Test Engineer guidance.
- Once PR is up, drop the link + test command here for acceptance review. Test Engineer to double-check PR includes the migrate prerequisite and logging evidence.

**Update 2025-12-17 â€” Full Stack Engineer**
- Branch pushed: `feature/LJ-006-ai-diagnostics-maths` and PR opened: <https://github.com/kingpius/learning_jungle/compare/feature/LJ-006-ai-diagnostics-maths?expand=1>. Description includes summary, acceptance mapping, determinism caveat (â€œseed enforced but true repeatability depends on provider; caching planned post-MVPâ€), and explicit reminder to run `python manage.py migrate` before tests. Test output attached: `python manage.py migrate && python manage.py test assessments rewards` (10 tests, green).
- Awaiting PM/Test review. Will monitor PR for feedback and keep aiservice.md + this ticket updated if prompts/logging change.
